{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.auto_scroll_threshold = 9999;"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn \n",
    "from sklearn.datasets import *\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder,StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier, BayesianRidge\n",
    "from sklearn.model_selection import RandomizedSearchCV,GridSearchCV\n",
    "from scipy.stats import randint as rint, uniform as runi\n",
    "from numpy.random import uniform as uni, randint as unint\n",
    "from sklearn.metrics import roc_curve,roc_auc_score,accuracy_score,confusion_matrix,precision_recall_curve\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Personal Classification repo\n",
    "from Garg_Classif_hyper import report_top_hyp,param_grid,models, \\\n",
    "                                feature_selection,hyperparam_search,imputers\n",
    "from Garg_Classif_roc import ROC_operating_point , ROC_optim_oper , final_mod_perf\n",
    "from Garg_Classif_plotting import ROC_plot , Precision_Recall_plot,outlier_analysis\n",
    "from Garg_Preprocessing import train_test_dev_split , outlier_det\n",
    "\n",
    "# Quick useful numpy functions\n",
    "roundnp = np.vectorize(lambda t: round(t,3))\n",
    "\n",
    "# Notebook Output viewing settings \n",
    "import warnings\n",
    "import math\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of all independent variables\n",
    "# Before inputting your dataset into dat_preprocess make sure no date time vars are there\n",
    "# Convert to numeric do feature engineering before proceeding further\n",
    "breast = load_breast_cancer()\n",
    "dat = breast.data\n",
    "tar = breast.target\n",
    "df = pd.DataFrame(data = dat,columns = breast.feature_names)\n",
    "tar = pd.DataFrame(data = tar,columns = ['outcome'])\n",
    "tar.loc[0:70,'outcome'] = 3\n",
    "df = pd.concat((df,tar),axis = 1)\n",
    "df['cat1'] = np.where(df['mean radius']>17,'hi','bye')\n",
    "df['cat2'] = df.apply(lambda x: 'to' if x['mean radius'] < 13 \n",
    "                      else np.nan if x['mean radius'] < 18 else 'no',axis = 1)\n",
    "df.iloc[0:25,0:2] = np.nan\n",
    "#df['date'] = pd.to_datetime('19000101', format='%Y%m%d', errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dat_preprocess(df,outcome_var,alow_miss_X_prcnt,*args):\n",
    "    \"\"\"\n",
    "    df - Complete DF including outcome var\n",
    "    args are the categorical predictors for one hot encoding specify in list format [v1,v2,..]\n",
    "    outcome_var = Just the name of the outcome var - eg : 'No_of_tickets'\n",
    "    \"\"\"\n",
    "    # Printing shape of the modelling set\n",
    "    print('We have %s examples to train/test on & %s independent variables' %(df.shape[0],df.shape[1]))\n",
    "    print (' ')\n",
    "    # One hot encoding categorical variables and output variable\n",
    "    try:\n",
    "        if args[0]:\n",
    "            data_set = pd.get_dummies(df,columns = args[0])\n",
    "    except:\n",
    "        print('No categorical predictors for one hot encoding provided')\n",
    "        data_set = df\n",
    "    \n",
    "    # Removing rows for Outcome variable with NAN values\n",
    "    print ('No of rows with outcome variable not populated : %d' \n",
    "           %(data_set[outcome_var].isna().sum()))\n",
    "           \n",
    "    data_set = data_set.loc[~data_set[outcome_var].isna(),:]\n",
    "    \n",
    "    print ('Orignal row count : %d' %(len(data_set)))\n",
    "    # Removing any duplicate rows\n",
    "    data_set.drop_duplicates(inplace = True)\n",
    "    print ('Deduplicated row count : %d' %(len(data_set)))\n",
    "    \n",
    "    #Checking and removing any predictors with >X% of the values as missing\n",
    "    per_miss_pred = ((data_set.isna().sum(axis = 0)/len(data_set))*100).round(1)\n",
    "    # Printing % missing value count if missing values > 0 %\n",
    "    print ('Variables with some missing values (ie > 0 percent):')\n",
    "    print (per_miss_pred.loc[per_miss_pred > 0])\n",
    "    if len(per_miss_pred.loc[per_miss_pred > alow_miss_X_prcnt]) == 0:\n",
    "        print ('No predictors with missing percent more than %d' %(alow_miss_X_prcnt))\n",
    "    else:\n",
    "        print('Removing the following predictors \\\n",
    "              as they have more than %d values missing' %(alow_miss_X_prcnt))\n",
    "        print(list(per_miss_pred.loc[per_miss_pred > alow_miss_X_prcnt].index))\n",
    "    \n",
    "    data_set_mod = data_set[per_miss_pred.loc[per_miss_pred <= alow_miss_X_prcnt].index]\n",
    "        \n",
    "    # Label encoding the Y variable\n",
    "    le = LabelEncoder()\n",
    "    data_set_mod[outcome_var] = le.fit_transform(data_set_mod[outcome_var])\n",
    "    print (' ')\n",
    "    print ('Encoding for the outcome variable')\n",
    "    le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "    print(le_name_mapping)\n",
    "    \n",
    "    # Splitting into train test and dev\n",
    "    X_train, X_dev, X_test, Y_train, Y_dev, Y_test = \\\n",
    "    train_test_dev_split(data_set_mod, outcome_var)\n",
    "    \n",
    "    # Distribution of the Y variable \n",
    "    print('Distribution of the Y variable')\n",
    "    print (round((Y_train.value_counts().sort_index()/Y_train.sum())*100,1))\n",
    "    return X_train, X_dev, X_test, Y_train, Y_dev, Y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_pipe(feat_selec,mod_class,imput_model,hyper_selc,rand_search_iter,scoring_crit,\n",
    "                    X_train,Y_train,X_dev,X_test,random_state):\n",
    "    \"\"\"\n",
    "    imput_model: Itterative imputation method model - BaysRidge(bays),RFregressor(Rf),Knn(Knn)\n",
    "    mod_class: either Tree_based , Stat_based\n",
    "    feat_selec: either KBest, L1 or PCA\n",
    "    hyper_selc: either Grid or Random\n",
    "    scoring_crit: Scoring criteria for top model f1,roc_auc,balanced_accuracy,precision,recall \n",
    "    (Use balanced_accuracy/accuracy for multiclass)\n",
    "    \"\"\"\n",
    "    scale_data = StandardScaler()\n",
    "    imputation = imputers(imput_model,random_state)\n",
    "    feat_selection_ = feature_selection(feat_selec)\n",
    "    mod_ = models(mod_class,random_state)\n",
    "    \n",
    "    hyp_search_res = {}\n",
    "    for mod_type,mod in mod_.items():\n",
    "        pipe = Pipeline(steps=[('scaled_data',scale_data),\n",
    "                               (imput_model,imputation),\n",
    "                               (feat_selec, feat_selection_), \n",
    "                               (mod_type, mod)])\n",
    "        \n",
    "        param = param_grid(feat_selec,mod_type,hyper_selc,rand_search_iter)\n",
    "        \n",
    "        search = hyperparam_search(hyper_selc,pipe,param,rand_search_iter,scoring_crit)\n",
    "        \n",
    "        search.fit(X_train, Y_train)\n",
    "        # Predict the prob of positive class for binary case else predict hard labels\n",
    "        if len(np.unique(Y_train)) == 2:\n",
    "            pred_dev = search.predict_proba(X_dev)[:,list(search.classes_).index(1)] \n",
    "            pred_test = search.predict_proba(X_test)[:,list(search.classes_).index(1)] \n",
    "        else:\n",
    "            pred_dev = search.predict(X_dev)\n",
    "            pred_test = search.predict(X_test)\n",
    "        print ('MODEL TYPE : %s' %(mod_type.upper()))\n",
    "        print ('')\n",
    "        report_top_hyp(search.cv_results_,scoring_crit)\n",
    "        print ('-----------------      ------------------')\n",
    "        print ('')\n",
    "        hyp_search_res[mod_type] = (search.best_params_,pred_dev,pred_test)\n",
    "    \n",
    "    return hyp_search_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_roc(Y_pred_prob,Y_act,fn_cost,fp_cost,Y_test_pred_prob):\n",
    "    \"\"\"\n",
    "    Y_pred_prob , Y_act : Pred prob and labels of the Dev set (from hyp param tuning step)\n",
    "    Y_test_pred_prob :  Pred prob and labels of the Test set (from hyp param tuning step) \n",
    "    Moves operating point up or down based on seperate costs of FP and FN and plot new ROC\n",
    "    Also plots precision recall plot for the new operating point\n",
    "    \"\"\"\n",
    "    random_pred = np.random.randint(0,2,len(Y_act))\n",
    "    fpr,tpr,thresholds = roc_curve(Y_act,Y_pred_prob,pos_label=1)\n",
    "    fpr_ran,tpr_ran,thresholds_ran = roc_curve(Y_act,random_pred,pos_label=1)\n",
    "    # Plotting best model and random classifier\n",
    "    print ('--------------')\n",
    "    print ('EVALUATING PERFORMANCE ON DEV SET')\n",
    "    print (' ')\n",
    "    print (\"The AUC area of the Best classifier is: %.2f\" %(roc_auc_score(Y_act,Y_pred_prob)))\n",
    "    print (\"The AUC area of a Random classifier is: %.2f\" %(roc_auc_score(Y_act,random_pred)))\n",
    "    \n",
    "    # Marking the DEFAULT operation point for the selected model - GBC    \n",
    "    AUC_char = pd.DataFrame(data = np.array([fpr,tpr,thresholds]).T,\n",
    "                     columns = ['Fpr','Tpr','Thresholds'])\n",
    "    \n",
    "    oper_fpr, oper_tpr = ROC_operating_point(AUC_char, 0.5)['Fpr'], \\\n",
    "    ROC_operating_point(AUC_char, 0.5)['Tpr']\n",
    "    \n",
    "    # Optimizing for the operating point \n",
    "    opt_thres,(opt_cost,opt_tpr,opt_fpr,def_cost) = ROC_optim_oper(Y_pred_prob,Y_act,fn_cost,fp_cost )\n",
    "    \n",
    "    # Plotting the ROC \n",
    "    plt.figure(figsize=(8, 12))\n",
    "    ROC_plot(fpr,tpr,oper_fpr,oper_tpr,opt_fpr,opt_tpr,fpr_ran,tpr_ran)\n",
    "    \n",
    "    # Plotting Precision Recall \n",
    "    Precision_Recall_plot(Y_act,Y_pred_prob,opt_thres)\n",
    "    \n",
    "    print('The new operating point is %.2f compared to the default of 0.50.' %(opt_thres))\n",
    "    print('This has reduced the overall cost of misclassification from %d to %d' %(def_cost,opt_cost))\n",
    "    pred_labels = np.vectorize(lambda x :1 if x > opt_thres else 0)\n",
    "    return pred_labels(Y_test_pred_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36 20 47] [7 2 2] [1 2 8] [13 33  0]\n",
      "[0.8372093  0.90909091 0.95918367] [0.1627907  0.09090909 0.04081633] [0.07142857 0.05714286 1.        ] [0.92857143 0.94285714 0.        ]\n",
      "  \n",
      "--------------\n",
      "PERFORMANCE ON UNTOUCHED TEST SET\n",
      "Performance of the post tuned classifier (Both hyperparameter and operating point) compared to the default classifier (ie if we randomly predicted positive/negetive based on a fair coin toss) :\n",
      "If there are 100 +ve cases our model is able to identify 62.38 of all the positive cases correctly while only misclassifying 37.62 of those cases as negative\n",
      "While on the other hand the random classifier is only able to identify 47.86 of all the positive cases correctly\n",
      " \n",
      "This is a LIFT of 1.3 times in predicting the positive class over the performance of the default classifier\n",
      " \n",
      "Similarly if there are 100 -ve cases our model is able to identify 90.18 of all the negative cases correctly while costing us very little by only misclassifying 9.82 of those cases as positive\n",
      "While on the other hand the random classifier costs us a lot (by misclasifying -ve as +ve) trying to predict the postive cases and misclassifies 28.31 of all the negative cases as positive\n"
     ]
    }
   ],
   "source": [
    "# X_train,X_dev, X_test,Y_train,Y_dev,Y_test = dat_preprocess(df,'outcome',30,['cat1','cat2'])\n",
    "\n",
    "# X_train_nooutlier,Y_train_nooutlier = \\\n",
    "# outlier_det(X_train,Y_train,'texture error','mean compactness','Robust covariance',0.05)   \n",
    "\n",
    "# outlier_analysis(X_train,X_train_nooutlier)\n",
    "\n",
    "# Train_res = make_train_pipe('PCA','Tree_based','bays','Random',10,'accuracy', \n",
    "#                             X_train_nooutlier,Y_train_nooutlier,X_dev,X_test,123)\n",
    "\n",
    "# Binary case\n",
    "# tuned_pred = tune_roc(Train_res['rf'][1],Y_dev,1,5,Train_res['rf'][2])\n",
    "#final_mod_perf(tuned_pred,Y_test,no_output_class = 2)\n",
    "\n",
    "# Multi class case\n",
    "final_mod_perf(Train_res['rf'][2],Y_test,no_output_class = 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_mod_perf(Y_pred,Y_act,no_output_class = 2):\n",
    "    \"\"\"\n",
    "    :param Y_pred: Y predicted value from classifier (Absolute class not probability) Can be multi class as well\n",
    "    :param Y_act: Actual labels\n",
    "    :param no_output_class: No of output classes in the output label\n",
    "    :return: Nothing\n",
    "    \"\"\"\n",
    "    Y_rand_pred = np.random.randint(0,no_output_class,len(Y_pred))\n",
    "    if no_output_class == 2:\n",
    "        tn, fp, fn, tp = confusion_matrix(Y_act, Y_pred).ravel()\n",
    "        tn_def, fp_def, fn_def, tp_def = confusion_matrix(Y_act, Y_rand_pred).ravel()\n",
    "    else:\n",
    "        tn, fp, fn, tp = multi_class_tn_fp(confusion_matrix(Y_act, Y_pred))\n",
    "        tn_def, fp_def, fn_def, tp_def = multi_class_tn_fp(confusion_matrix(Y_act, Y_rand_pred))\n",
    "    \n",
    "    print (tn, fp, fn, tp)\n",
    "    tnr, fpr, fnr, tpr = get_tpr_4(tn, fp, fn, tp )\n",
    "    tnr_def, fpr_def, fnr_def, tpr_def = get_tpr_4(tn_def, fp_def, fn_def, tp_def)\n",
    "    print (tnr, fpr, fnr, tpr )\n",
    "    # Averaging tpr, fpr etc for multi class case across all classes \n",
    "    if no_output_class != 2:   \n",
    "        tnr, fpr, fnr, tpr,tnr_def, fpr_def, fnr_def, tpr_def = \\\n",
    "        map(lambda x:np.mean(x) ,(tnr, fpr, fnr, tpr,tnr_def, fpr_def, fnr_def, tpr_def))\n",
    "    print ('  ')\n",
    "    print ('--------------')\n",
    "    print ('PERFORMANCE ON UNTOUCHED TEST SET')\n",
    "    print ('Performance of the post tuned classifier (Both hyperparameter and operating point) compared to the default classifier (ie if we randomly predicted positive/negetive based on a fair coin toss) :' )\n",
    "    print ('If there are 100 +ve cases our model is able to identify %s of all the positive cases correctly while only misclassifying %s of those cases as negative' %(round(tpr*100,2),round(fnr*100,2)))\n",
    "    print ('While on the other hand the random classifier is only able to identify %s of all the positive cases correctly' %(round(tpr_def*100,2)))\n",
    "    print (' ')\n",
    "    print ('This is a LIFT of %s times in predicting the positive class over the performance of the default classifier' %(round(tpr/tpr_def,1)))\n",
    "    print (' ')\n",
    "    print('Similarly if there are 100 -ve cases our model is able to identify %s of all the negative cases correctly while costing us very little by only misclassifying %s of those cases as positive' % (round(tnr*100,2),round(fpr*100,2)))\n",
    "    print ('While on the other hand the random classifier costs us a lot (by misclasifying -ve as +ve) trying to predict the postive cases and misclassifies %s of all the negative cases as positive' %(round(fpr_def*100,2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mod_perf(Train_res['svm'][2],Y_test,no_output_class = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = \n",
    "print (a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tpr_4(tn,fp,fn,tp):\n",
    "    \"\"\"\n",
    "    :param tn: TN\n",
    "    :param fp: FP\n",
    "    :param fn: FN\n",
    "    :param tp: TP\n",
    "    :return: All 4 TNR, FPR, FNR, TPR\n",
    "    \"\"\"\n",
    "    return tn/(tn + fp) ,fp/(fp + tn) ,fn/(fn + tp) ,tp/(tp + fn)\n",
    "\n",
    "def multi_class_tn_fp(confusion_matrix):\n",
    "    \"\"\"\n",
    "    :param confusion_matrix: Take in a multi class >2 confusion matrix\n",
    "    :return: tn,fp,fn,tp values\n",
    "    \"\"\"\n",
    "    fp = confusion_matrix.sum(axis=0) - np.diag(confusion_matrix)\n",
    "    fn = confusion_matrix.sum(axis=1) - np.diag(confusion_matrix)\n",
    "    tp = np.diag(confusion_matrix)\n",
    "    tn = confusion_matrix.sum() - (fp + fn + tp)\n",
    "    return tn,fp,fn,tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "36/37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([1 ,1 ,1])/ (np.array([1, 1, 1]) + np.array([2, 2 ,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
